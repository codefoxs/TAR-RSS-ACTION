import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timezone
import xml.etree.ElementTree as ET
import sys

def generate_rss():
    # 1. ä»ç¯å¢ƒå˜é‡è·å– API Key
    api_key = os.getenv("scraper_api")
    if not api_key:
        print("âŒ é”™è¯¯: è¯·åœ¨ GitHub Secrets ä¸­é…ç½® scraper_api")
        sys.exit(1)

    # 2. å‡†å¤‡è¯·æ±‚å‚æ•°
    # å¦‚æœåç»­å‘ç°æŠ“ä¸åˆ°æ–‡ç« ï¼Œå¯ä»¥åœ¨ payload å¢åŠ  'render': 'true'
    target_url = "https://publications.aaahq.org/accounting-review/publish-ahead-of-print"
    payload = { 
        'api_key': api_key, 
        'url': target_url,
        'keep_headers': 'true' # å»ºè®®åŠ ä¸Šï¼Œè®© ScraperAPI æ›´å¥½åœ°å¤„ç†è¯·æ±‚å¤´
    }

    print(f"ğŸš€ æ­£åœ¨é€šè¿‡ ScraperAPI è¯·æ±‚: {target_url}")
    
    try:
        # ä½¿ç”¨ä½ æåˆ°çš„ api.scraperapi.com æ¥å£
        r = requests.get('https://api.scraperapi.com/', params=payload, timeout=60)
        r.raise_for_status()
        html = r.text
        print("âœ… é¡µé¢æŠ“å–æˆåŠŸ!")
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥: {e}")
        sys.exit(1)

    # 3. è§£æ HTML
    soup = BeautifulSoup(html, "lxml")
    articles = soup.select("div.al-article-items")
    
    if not articles:
        print("âš ï¸ æœªæ‰¾åˆ°æ–‡ç« å…ƒç´ ã€‚")
        # è°ƒè¯•ï¼šçœ‹çœ‹æ˜¯ä¸æ˜¯è¢«æ‹¦æˆªäº†
        if "captcha" in html.lower() or "robot" in html.lower():
            print("ğŸ›‘ ä¼¼ä¹é‡åˆ°äº†éªŒè¯ç ï¼Œå¯èƒ½éœ€è¦å¢åŠ  render: true å‚æ•°")
        return

    # 4. åˆ›å»º RSS ç»“æ„
    rss = ET.Element("rss", version="2.0")
    channel = ET.SubElement(rss, "channel")

    BASE_URL = "https://publications.aaahq.org"
    ET.SubElement(channel, "title").text = "The Accounting Review â€“ Early Access"
    ET.SubElement(channel, "link").text = target_url
    ET.SubElement(channel, "description").text = "Early access articles from The Accounting Review"
    ET.SubElement(channel, "language").text = "en-us"
    ET.SubElement(channel, "lastBuildDate").text = datetime.now(timezone.utc).strftime(
        "%a, %d %b %Y %H:%M:%S GMT"
    )

    # 5. é€ç¯‡æ–‡ç« å†™å…¥ item
    count = 0
    for art in articles:
        title_tag = art.select_one("h5.al-title a")
        if not title_tag: continue

        title = title_tag.get_text(strip=True)
        link = BASE_URL + title_tag["href"] if title_tag["href"].startswith('/') else title_tag["href"]

        authors_block = art.select_one(".al-authors-list")
        authors = authors_block.get_text(separator="", strip=True) if authors_block else "Authors not listed"

        pub_date_tag = art.select_one(".al-pub-date")
        pub_date = pub_date_tag.get_text(strip=True) if pub_date_tag else ""

        item = ET.SubElement(channel, "item")
        ET.SubElement(item, "title").text = title
        ET.SubElement(item, "link").text = link
        ET.SubElement(item, "guid").text = link
        ET.SubElement(item, "description").text = f"<b>Authors:</b> {authors}<br/>{pub_date}"
        count += 1

    # 6. è¾“å‡º rss.xml
    tree = ET.ElementTree(rss)
    tree.write("tar.xml", encoding="utf-8", xml_declaration=True)
    print(f"ğŸ‰ å®Œæˆ! å·²ç”Ÿæˆ {count} ç¯‡æ–‡ç« ")

if __name__ == "__main__":
    generate_rss()
