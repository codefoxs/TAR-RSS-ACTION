import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timezone
import xml.etree.ElementTree as ET
import sys

def generate_rss():
    # 1. ä»ç¯å¢ƒå˜é‡è·å– API Key (å®‰å…¨ç¬¬ä¸€)
    api_key = os.getenv("SCRAPERAPI_KEY")
    if not api_key:
        print("âŒ é”™è¯¯: è¯·åœ¨ GitHub Secrets ä¸­é…ç½® SCRAPERAPI_KEY")
        sys.exit(1)

    target_url = "https://publications.aaahq.org/accounting-review/publish-ahead-of-print"
    
    # 2. æ„å»º ScraperAPI ä»£ç†å‚æ•°
    # render=true è¡¨ç¤ºè®© ScraperAPI å¸®æˆ‘ä»¬è¿è¡Œ JSï¼ˆç±»ä¼¼ Selenium çš„æ•ˆæœï¼‰
    proxy_url = "http://scraperapi:{}@proxy-server.scraperapi.com:8001".format(api_key)
    proxies = {
        "http": proxy_url,
        "https": proxy_url
    }

    print(f"ğŸš€ æ­£åœ¨é€šè¿‡ ScraperAPI è¯·æ±‚é¡µé¢: {target_url}")
    
    try:
        # å³ä½¿ç›®æ ‡ç«™æ˜¯ HTTPSï¼Œé€šè¿‡ä»£ç†è¯·æ±‚æ—¶é€šå¸¸å»ºè®® verify=False æˆ–ä½¿ç”¨å…¶æä¾›çš„ CA
        response = requests.get(target_url, proxies=proxies, verify=False, timeout=60)
        response.raise_for_status()
        html = response.text
        print("âœ… é¡µé¢æŠ“å–æˆåŠŸ!")
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥: {e}")
        sys.exit(1)

    # 3. è§£æ HTML
    soup = BeautifulSoup(html, "lxml")
    articles = soup.select("div.al-article-items")
    
    if not articles:
        print("âš ï¸ æœªæ‰¾åˆ°æ–‡ç« å…ƒç´ ï¼Œå¯èƒ½æ˜¯é¡µé¢ç»“æ„å˜åŒ–æˆ–è¢«æ‹¦æˆªã€‚")
        # æ‰“å°éƒ¨åˆ†æºç ä»¥ä¾¿åœ¨ Action æ—¥å¿—ä¸­æ’æŸ¥
        print(html[:500])
        return

    # 4. åˆ›å»º RSS ç»“æ„
    rss = ET.Element("rss", version="2.0")
    channel = ET.SubElement(rss, "channel")

    BASE_URL = "https://publications.aaahq.org"
    ET.SubElement(channel, "title").text = "The Accounting Review â€“ Early Access"
    ET.SubElement(channel, "link").text = target_url
    ET.SubElement(channel, "description").text = "Early access articles from The Accounting Review"
    ET.SubElement(channel, "language").text = "en-us"
    ET.SubElement(channel, "lastBuildDate").text = datetime.now(timezone.utc).strftime(
        "%a, %d %b %Y %H:%M:%S GMT"
    )

    # 5. é€ç¯‡æ–‡ç« å†™å…¥ item
    count = 0
    for art in articles:
        title_tag = art.select_one("h5.al-title a")
        if not title_tag:
            continue

        title = title_tag.get_text(strip=True)
        link = BASE_URL + title_tag["href"]

        authors_block = art.select_one(".al-authors-list")
        authors = authors_block.get_text(separator="", strip=True) if authors_block else "Authors not listed"

        pub_date_tag = art.select_one(".al-pub-date")
        pub_date = pub_date_tag.get_text(strip=True) if pub_date_tag else ""

        item = ET.SubElement(channel, "item")
        ET.SubElement(item, "title").text = title
        ET.SubElement(item, "link").text = link
        ET.SubElement(item, "guid").text = link
        ET.SubElement(item, "description").text = f"<b>Authors:</b> {authors}<br/>{pub_date}"
        count += 1

    # 6. è¾“å‡º rss.xml
    tree = ET.ElementTree(rss)
    tree.write("tar.xml", encoding="utf-8", xml_declaration=True)
    print(f"ğŸ‰ å®Œæˆ! å·²ç”ŸæˆåŒ…å« {count} ç¯‡æ–‡ç« çš„ tar.xml")

if __name__ == "__main__":
    generate_rss()
