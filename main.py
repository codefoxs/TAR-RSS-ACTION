import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from datetime import datetime, timezone
import xml.etree.ElementTree as ET
import time
import sys

# 1. é…ç½® undetected_chromedriver
options = uc.ChromeOptions()
options.add_argument("--headless")  # GitHub Actions å¿…é¡»å¼€å¯ headless
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
options.add_argument("--window-size=1920,1080")

# æ³¨æ„ï¼šuc å†…éƒ¨ä¼šè‡ªåŠ¨ç”Ÿæˆåˆé€‚çš„ User-Agentï¼Œé€šå¸¸ä¸éœ€è¦æ‰‹åŠ¨åŠ  fake-useragent
# ä½†å¦‚æœæƒ³æ›´ç¨³å¦¥ï¼Œå¯ä»¥ä¿ç•™è¿™è¡Œï¼Œä½† uc é»˜è®¤çš„å·²ç»å¾ˆå¼ºäº†
# options.add_argument(f"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

try:
    print("ğŸš€ æ­£åœ¨å¯åŠ¨ undetected_chromedriver...")
    driver = uc.Chrome(options=options)
    
    print("ğŸŒ æ­£åœ¨è®¿é—®é¡µé¢...")
    driver.get("https://publications.aaahq.org/accounting-review/publish-ahead-of-print")

    # æ¨¡æ‹Ÿäººç±»éšæœºç­‰å¾… 3-5 ç§’ï¼Œè®©é¡µé¢è„šæœ¬è¿è¡Œ
    time.sleep(5)

    print("â³ ç­‰å¾…é¡µé¢å…ƒç´ åŠ è½½...")
    # å¢åŠ åˆ° 30 ç§’è¶…æ—¶ï¼Œå¹¶åœ¨å¤±è´¥æ—¶æ•è·é”™è¯¯
    wait = WebDriverWait(driver, 30)
    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.al-article-items")))

    html = driver.page_source
    print("âœ… é¡µé¢åŠ è½½æˆåŠŸï¼Œå¼€å§‹è§£æ...")

except Exception as e:
    print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
    # è°ƒè¯•å…³é”®ï¼šæŠ¥é”™æ—¶æ‰“å°é¡µé¢æ ‡é¢˜ï¼Œçœ‹æ˜¯å¦è¢«æ‹¦æˆªï¼ˆå¦‚æ˜¾ç¤º 403 Forbiddenï¼‰
    if 'driver' in locals():
        print(f"å½“å‰é¡µé¢æ ‡é¢˜: {driver.title}")
        # å¦‚æœè¢«æ‹¦æˆªï¼Œå¯ä»¥ä¿å­˜æºç æŸ¥çœ‹åŸå› 
        # with open("error_debug.html", "w", encoding="utf-8") as f:
        #     f.write(driver.page_source)
    sys.exit(1)

# --- ä»¥ä¸‹é€»è¾‘ä¿æŒä¸å˜ ---

soup = BeautifulSoup(html, "lxml")
articles = soup.select("div.al-article-items")

rss = ET.Element("rss", version="2.0")
channel = ET.SubElement(rss, "channel")

BASE_URL = "https://publications.aaahq.org"
ET.SubElement(channel, "title").text = "The Accounting Review â€“ Early Access"
ET.SubElement(channel, "link").text = BASE_URL + "/accounting-review/publish-ahead-of-print"
ET.SubElement(channel, "description").text = "Early access articles from The Accounting Review"
ET.SubElement(channel, "language").text = "en-us"
ET.SubElement(channel, "lastBuildDate").text = datetime.now(timezone.utc).strftime(
    "%a, %d %b %Y %H:%M:%S GMT"
)

for art in articles:
    title_tag = art.select_one("h5.al-title a")
    if not title_tag:
        continue

    title = title_tag.get_text(strip=True)
    link = BASE_URL + title_tag["href"]

    authors_block = art.select_one(".al-authors-list")
    authors = authors_block.get_text(
        separator="", strip=True
    ) if authors_block else "Authors not listed"

    pub_date_tag = art.select_one(".al-pub-date")
    pub_date = pub_date_tag.get_text(strip=True) if pub_date_tag else ""

    item = ET.SubElement(channel, "item")
    ET.SubElement(item, "title").text = title
    ET.SubElement(item, "link").text = link
    ET.SubElement(item, "guid").text = link
    ET.SubElement(item, "description").text = (
        f"<b>Authors:</b> {authors}<br/>{pub_date}"
    )

tree = ET.ElementTree(rss)
tree.write("tar.xml", encoding="utf-8", xml_declaration=True)

driver.quit()
print("ğŸ‰ tar.xml å·²æˆåŠŸç”Ÿæˆ")
