import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timezone
import xml.etree.ElementTree as ET
import sys
from fake_useragent import UserAgent  # å¯¼å…¥åº“

def get_random_headers():
    """ç”Ÿæˆéšæœºè¯·æ±‚å¤´"""
    ua = UserAgent()
    return {
        'User-Agent': ua.random,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Connection': 'keep-alive',
    }

def generate_rss():
    # 1. ä»ç¯å¢ƒå˜é‡è·å– API Key
    api_key = os.getenv("SCRAPER_API")
    if not api_key:
        print("âŒ é”™è¯¯: è¯·åœ¨ GitHub Secrets ä¸­é…ç½® SCRAPER_API")
        sys.exit(1)

    # 2. å‡†å¤‡è¯·æ±‚å‚æ•°
    target_url = "https://publications.aaahq.org/accounting-review/publish-ahead-of-print"
    
    # è·å–éšæœº Header
    headers = get_random_headers()
    
    payload = { 
        'api_key': api_key, 
        'url': target_url,
        'keep_headers': 'true' # å‘Šè¯‰ ScraperAPI è½¬å‘æˆ‘ä»¬è‡ªå®šä¹‰çš„ headers
    }

    print(f"ğŸš€ æ­£åœ¨é€šè¿‡ ScraperAPI è¯·æ±‚: {target_url}")
    print(f"ğŸ•µï¸ ä½¿ç”¨ä¼ªè£… User-Agent: {headers['User-Agent']}")
    
    try:
        # å°† headers ä¼ å…¥ requests
        r = requests.get(
            'https://api.scraperapi.com/', 
            params=payload, 
            headers=headers, 
            timeout=60
        )
        r.raise_for_status()
        html = r.text
        print("âœ… é¡µé¢æŠ“å–æˆåŠŸ!")
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥: {e}")
        sys.exit(1)

    # 3. è§£æ HTML
    soup = BeautifulSoup(html, "lxml")
    articles = soup.select("div.al-article-items")
    
    if not articles:
        print("âš ï¸ æœªæ‰¾åˆ°æ–‡ç« å…ƒç´ ã€‚")
        if "captcha" in html.lower() or "robot" in html.lower():
            print("ğŸ›‘ ä¼¼ä¹é‡åˆ°äº†éªŒè¯ç ã€‚ScraperAPI å»ºè®®å°è¯•å¢åŠ  'render': 'true' å‚æ•°")
        return

    # 4. åˆ›å»º RSS ç»“æ„
    rss = ET.Element("rss", version="2.0")
    channel = ET.SubElement(rss, "channel")

    BASE_URL = "https://publications.aaahq.org"
    ET.SubElement(channel, "title").text = "The Accounting Review â€“ Early Access"
    ET.SubElement(channel, "link").text = target_url
    ET.SubElement(channel, "description").text = "Early access articles from The Accounting Review"
    ET.SubElement(channel, "language").text = "en-us"
    ET.SubElement(channel, "lastBuildDate").text = datetime.now(timezone.utc).strftime(
        "%a, %d %b %Y %H:%M:%S GMT"
    )

    # 5. é€ç¯‡æ–‡ç« å†™å…¥ item
    count = 0
    for art in articles:
        title_tag = art.select_one("h5.al-title a")
        if not title_tag: continue

        title = title_tag.get_text(strip=True)
        link = BASE_URL + title_tag["href"] if title_tag["href"].startswith('/') else title_tag["href"]

        authors_block = art.select_one(".al-authors-list")
        authors = authors_block.get_text(separator="", strip=True) if authors_block else "Authors not listed"

        pub_date_tag = art.select_one(".al-pub-date")
        pub_date = pub_date_tag.get_text(strip=True) if pub_date_tag else ""

        item = ET.SubElement(channel, "item")
        ET.SubElement(item, "title").text = title
        ET.SubElement(item, "link").text = link
        ET.SubElement(item, "guid").text = link
        ET.SubElement(item, "description").text = f"<b>Authors:</b> {authors}<br/>{pub_date}"
        count += 1

    # 6. è¾“å‡º tar.xml
    tree = ET.ElementTree(rss)
    tree.write("tar.xml", encoding="utf-8", xml_declaration=True)
    print(f"ğŸ‰ å®Œæˆ! å·²ç”Ÿæˆ {count} ç¯‡æ–‡ç« ")

if __name__ == "__main__":
    generate_rss()
